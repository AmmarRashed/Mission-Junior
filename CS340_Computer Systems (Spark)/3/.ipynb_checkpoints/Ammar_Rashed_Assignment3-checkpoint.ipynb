{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">CS340 - Assignment 3</h1>\n",
    "<h3 align=\"center\">Due Date: 30 April 2017</h3>\n",
    "<br>\n",
    "<p style=\"text-indent: 40px\">In this project we are going to use a clustering method(LDA) on the Yelp dataset. The dataset is included in the assignment folder. You should unzip and upload the file to ibm's datascience tool.\n",
    "</p>\n",
    "\n",
    "\n",
    "##Â Topics From Reviews\n",
    "\n",
    "\"LDA is a topic model which infers topics from a collection of text documents.\"\n",
    "For the purpose of this assignment we can treat each yelp review as a document and extract two topics from the data by using LDA.\n",
    "\n",
    "For this assignment you should use ml transformers and ml pipeline. Just for convenince, ReviewsRdd is provided for you. You should convert this rdd to DataFrame and work on it. In order to do this assignment you should revisit PySpark ML documentation many times.\n",
    "\n",
    "End result should look like this.\n",
    "\n",
    "<div>\n",
    "    <img src=\"http://image.prntscr.com/image/13eb8a01533346c0bb3186dfc5402b73.png\" width=200>\n",
    "</div>\n",
    "\n",
    "Your solution does not have to be exactly like this, since each time you run the LDA model, it gives you different results. \n",
    "\n",
    "<p style=\"text-indent: 40px\"> Note: You should remove stopwords with a feature transformer, the words you should remove are in the stopwords list that we have provided.</p>\n",
    "<p style=\"text-indent: 40px\">Important: Do not discuss the solution with your friends. <b>Plagiarism</b> will not be tolerated and issue will be referred to the <b>disciplinary committee</b>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "# @hidden_cell\n",
    "# This function is used to setup the access of Spark to your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# The following variable contains the path to your file on your Object Storage.\n",
    "path_1 = \"swift://CS340.\" + name + \"/100kReviews.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\",\"i'll\",\"you'll\",\"he'll\",\"she'll\",\"we'll\",\"they'll\",\"i'd\",\"you'd\",\"he'd\",\"she'd\",\"we'd\",\"they'd\",\"i'm\",\"you're\",\"he's\",\"she's\",\"it's\",\"we're\",\"they're\",\"i've\",\"we've\",\"you've\",\"they've\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"don't\",\"doesn't\",\"didn't\",\"won't\",\"wouldn't\",\"shan't\",\"shouldn't\",\"mustn't\",\"can't\",\"couldn't\",\"cannot\",\"could\",\"here's\",\"how's\",\"let's\",\"ought\",\"that's\",\"there's\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\"]\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "# convert all words to lowercase\n",
    "# remove words that are too long or too short \n",
    "reviewsRdd = sc.textFile(path_1)\\\n",
    "            .map(lambda line: line.lower())\\\n",
    "            .map(lambda line: \" \".join(word for word in line.strip().split() if 25 > len(word) > 4))\\\n",
    "            .zipWithIndex()\\\n",
    "            .map(lambda (text, indx):Row(Id=indx, Text=text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = sqlContext.createDataFrame(reviewsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.ml.linalg import Vector, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline\n",
    "tokenizer = Tokenizer(inputCol='Text', outputCol='Words')\n",
    "removeStopWords = StopWordsRemover(inputCol='Words', outputCol='filtered_words', stopWords=stopwords)\n",
    "count_vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='vectors')\n",
    "words_per_topic = 6\n",
    "num_of_topics = 2\n",
    "lda_model = LDA(featuresCol='vectors', k=num_of_topics)\n",
    "pipeline = Pipeline(stages=[tokenizer, removeStopWords, count_vectorizer, lda_model])\n",
    "\n",
    "model = pipeline.fit(reviews)\n",
    "cv = model.stages[2]\n",
    "all_vocab = cv.vocabulary\n",
    "lda = model.stages[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_indices = lda.describeTopics(words_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[46, 208, 241, 28...|[0.00700437883614...|\n",
      "|    1|  [0, 1, 2, 3, 4, 5]|[0.00842281832305...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_indices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic0\n",
      "nicht\n",
      "einen\n",
      "essen\n",
      "waren\n",
      "einem\n",
      "etwas\n",
      "\n",
      "Topic1\n",
      "place\n",
      "great\n",
      "really\n",
      "service\n",
      "always\n",
      "little\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rendering the topics and their terms\n",
    "topics = topics_indices.select(\"termIndices\").rdd\\\n",
    "                     .map(lambda row:[all_vocab[row.termIndices[i]] for i in range(words_per_topic)]).collect()\n",
    "\n",
    "for topic in range(len(topics)):\n",
    "    print \"Topic\" + str(topic)\n",
    "    for term in topics[topic]:\n",
    "        print term\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.transform(reviews).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
